{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OngXDs1f6FeQ",
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mTN6FBT6FeQ"
   },
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33AY1H4N6FeR"
   },
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LYEkLii6FeR"
   },
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MFwtvsgY6FeR",
    "outputId": "2ed2d9bd-ded9-499b-cd29-7affb8b95b00"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\slepnev.py\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\slepnev.py\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\slepnev.py\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\slepnev.py\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\slepnev.py\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import warnings\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "XAmrKtle6FeT",
    "outputId": "998ac163-4bb8-467f-f6c8-09e3bb399f54"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0\n",
       "5           5  \"\\n\\nCongratulations from me as well, use the ...      0\n",
       "6           6       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "7           7  Your vandalism to the Matt Shirvington article...      0\n",
       "8           8  Sorry if the word 'nonsense' was offensive to ...      0\n",
       "9           9  alignment on this subject and which are contra...      0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    data = pd.read_csv('toxic_comments.csv')\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aL9fwamj6FeU"
   },
   "source": [
    "Выводы:\n",
    "1. Текст на английсокм языке\n",
    "2. Текст необходимо леммитизировать и очистить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "HDwI19KI6FeU"
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatize(text):\n",
    "    s = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(text)]\n",
    "    return(' '.join(s))\n",
    "\n",
    "\n",
    "def clear_text(text):\n",
    "    pattern = r'[^a-zA-Z]'\n",
    "    text = re.sub(pattern, \" \", text)\n",
    "    \n",
    "    return \" \".join(text.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bSj7FdZ374bu",
    "outputId": "74d80031-bba3-4a4d-920b-762cda3c3e4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 23s, sys: 1min 21s, total: 22min 45s\n",
      "Wall time: 22min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['lemma'] = data['text'].apply(lambda x: lemmatize(clear_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYQLsdAsykRN",
    "outputId": "219322b9-46fc-47bf-802b-b872ffed1c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "#Посмотрим количество Nan (если в ячейке были толкьо символы)\n",
    "print(data['lemma'].isna().sum())\n",
    "#уберем\n",
    "data = data[data['lemma'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dbrSNiEU6FeV"
   },
   "outputs": [],
   "source": [
    "#Скорее всего, придется рассматривать несколько моделей, разобъем на train, valid, test\n",
    "_,test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train, valid = train_test_split(_, test_size=0.1, random_state=42)\n",
    "X_train, y_train = train['lemma'], train['toxic']\n",
    "X_valid, y_valid = valid['lemma'], valid['toxic']\n",
    "X_test, y_test = test['lemma'], test['toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_Y9UjTM6FeW"
   },
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "6zjIFTKiH2tF",
    "outputId": "3827e29c-1eed-49ca-81cd-02b2b9a3e937"
   },
   "outputs": [],
   "source": [
    "#Попробуем использовать 2 модели.\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "pipe_lr = Pipeline([('vectorizer', TfidfVectorizer(stop_words=stopwords)),\n",
    "                    ('LR', LogisticRegression(random_state=42))])\n",
    "\n",
    "pipe_knn = Pipeline([('vectorizer', TfidfVectorizer(stop_words=stopwords)),\n",
    "                    ('knn', KNeighborsClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "jLt-5zp0s-4x"
   },
   "outputs": [],
   "source": [
    "lr_param_grid = [{'LR__max_iter': [75, 100, 120],\n",
    "                  'LR__C' :[5, 10, 20]}]\n",
    "\n",
    "knn_param_grid = [{'knn__n_neighbors': [3, 5]}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "O9FVAbwJLzCv"
   },
   "outputs": [],
   "source": [
    "cv = KFold(3, shuffle=True, random_state=42)\n",
    "lr_grid_search = GridSearchCV(estimator=pipe_lr,\n",
    "                 param_grid=lr_param_grid,\n",
    "                 scoring='f1',\n",
    "                 cv=cv)\n",
    "\n",
    "knn_grid_search = GridSearchCV(estimator=pipe_knn,\n",
    "                         param_grid=knn_param_grid,\n",
    "                         scoring='f1',\n",
    "                         cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xt6s9FCUul-3",
    "outputId": "cb752acd-74af-447c-9eed-0c2959621066"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 9min 37s\n",
      "Wall time: 3min 44s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=3, random_state=42, shuffle=True),\n",
       "             estimator=Pipeline(steps=[('vectorizer',\n",
       "                                        TfidfVectorizer(stop_words={'a',\n",
       "                                                                    'about',\n",
       "                                                                    'above',\n",
       "                                                                    'after',\n",
       "                                                                    'again',\n",
       "                                                                    'against',\n",
       "                                                                    'ain',\n",
       "                                                                    'all', 'am',\n",
       "                                                                    'an', 'and',\n",
       "                                                                    'any',\n",
       "                                                                    'are',\n",
       "                                                                    'aren',\n",
       "                                                                    \"aren't\",\n",
       "                                                                    'as', 'at',\n",
       "                                                                    'be',\n",
       "                                                                    'because',\n",
       "                                                                    'been',\n",
       "                                                                    'before',\n",
       "                                                                    'being',\n",
       "                                                                    'below',\n",
       "                                                                    'between',\n",
       "                                                                    'both',\n",
       "                                                                    'but', 'by',\n",
       "                                                                    'can',\n",
       "                                                                    'couldn',\n",
       "                                                                    \"couldn't\", ...})),\n",
       "                                       ('LR',\n",
       "                                        LogisticRegression(random_state=42))]),\n",
       "             param_grid=[{'LR__C': [5, 10, 20],\n",
       "                          'LR__max_iter': [75, 100, 120]}],\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lr_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LR__C': 20, 'LR__max_iter': 120}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zr1xkuaq8YNg",
    "outputId": "a8183d44-7b04-402b-fcfd-83581d232f97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7621527777777778\n"
     ]
    }
   ],
   "source": [
    "#Посмотрим результат на валидационной выборке\n",
    "pred = lr_grid_search.best_estimator_.predict(X_valid)\n",
    "score = f1_score(y_valid, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "id": "DyCMoCRyNMK0",
    "outputId": "be846e26-ccf4-49bf-e069-ac646d6196be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10min 3s\n",
      "Wall time: 10min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=3, random_state=42, shuffle=True),\n",
       "             estimator=Pipeline(steps=[('vectorizer',\n",
       "                                        TfidfVectorizer(stop_words={'a',\n",
       "                                                                    'about',\n",
       "                                                                    'above',\n",
       "                                                                    'after',\n",
       "                                                                    'again',\n",
       "                                                                    'against',\n",
       "                                                                    'ain',\n",
       "                                                                    'all', 'am',\n",
       "                                                                    'an', 'and',\n",
       "                                                                    'any',\n",
       "                                                                    'are',\n",
       "                                                                    'aren',\n",
       "                                                                    \"aren't\",\n",
       "                                                                    'as', 'at',\n",
       "                                                                    'be',\n",
       "                                                                    'because',\n",
       "                                                                    'been',\n",
       "                                                                    'before',\n",
       "                                                                    'being',\n",
       "                                                                    'below',\n",
       "                                                                    'between',\n",
       "                                                                    'both',\n",
       "                                                                    'but', 'by',\n",
       "                                                                    'can',\n",
       "                                                                    'couldn',\n",
       "                                                                    \"couldn't\", ...})),\n",
       "                                       ('knn', KNeighborsClassifier())]),\n",
       "             param_grid=[{'knn__n_neighbors': [3, 5]}], scoring='f1')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "knn_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knn__n_neighbors': 3}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mOyQHG9yTYZg",
    "outputId": "2c7b0d49-3776-4235-8680-9da57a6815c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3217665615141956\n"
     ]
    }
   ],
   "source": [
    "#Посмотрим результат на валидационной выборке\n",
    "pred = knn_grid_search.best_estimator_.predict(X_valid)\n",
    "score = f1_score(y_valid, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: Логистическая регрессия показала лучшие результаты на валидационной выборке, ее и будем проверять на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "7p0BTDwC6FeY",
    "outputId": "3ea7494e-f236-4533-a1a6-270c321e64ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7762867334805503\n"
     ]
    }
   ],
   "source": [
    "#Проверим на тестовой выборке\n",
    "pred = lr_grid_search.best_estimator_.predict(X_test)\n",
    "score = f1_score(y_test, pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veSERX4t6FeZ"
   },
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vo7jyolz6FeZ"
   },
   "source": [
    "1. Для леммитизации текста очень хорошо себя показала модель pymorphy2.MorphAnalyzer(). Файл обработался за 3 минуты.\n",
    "2. Для обработки такого файла на локальном компьютере у меня подошла только модель логистической регрессии и метод ближайших соседей. Никакими другими моделями достичь результата не получилось. Необходимы расширенные вычислительные мощности\n",
    "3. Удалось достичь требуемого результата - метрика F1 свыше 0,75.Улучшить модель удалось через введение коэффициента регуляризации. С увеличением \"штрафа\" за сложность модели вырасает ее точность. Но разница между С=10 и С=20 незначительная.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLpJGZGx6FeZ"
   },
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AN-WlBX6FeZ"
   },
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [X]  Весь код выполняется без ошибок\n",
    "- [X]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [X]  Данные загружены и подготовлены\n",
    "- [X]  Модели обучены\n",
    "- [X]  Значение метрики *F1* не меньше 0.75\n",
    "- [X]  Выводы написаны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_AmgaQ96Fea"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
